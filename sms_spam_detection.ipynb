{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load in csv dataset\n",
    "df = pd.read_csv('data/spam.csv',delimiter=',',encoding='latin-1')\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of ham and spam messages')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See output distribution\n",
    "sns.countplot(x=df.v1)\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of ham and spam messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and outputs\n",
    "X = df.v2 #.iloc[:300]\n",
    "Y = df.v1 #.iloc[:300]\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\n",
    "#,Y_train,Y_test = \n",
    "# print(len(X_train))\n",
    "# print(len(Y_train))\n",
    "# print(len(Y_test))\n",
    "# print(len(Y_test)/(len(Y_train)+len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen = len(max(X_train, key=lambda x: len(x.split())).split())\n",
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions taken from emo_utils.py\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    print('Y.reshape(-1)',Y.reshape(-1))\n",
    "    print('C',C)\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "def read_csv(filename = 'data/emojify_data.csv'):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji, dtype=int)\n",
    "\n",
    "    return X[:10], Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OG Data\n",
    "X_train, Y_train = read_csv('og_data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('og_data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen = len(max(X_train, key=lambda x: len(x.split())).split())\n",
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_NP = np.array(Y_train)\n",
    "Y_test_NP = np.array(Y_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "vec = label_encoder.fit_transform(Y_train_NP)\n",
    "print(np.unique(vec))\n",
    "print(np.unique(Y_train_NP))\n",
    "\n",
    "# Y_oh_train = pd.get_dummies(Y_train)\n",
    "# Y_oh_test = pd.get_dummies(Y_test)\n",
    "# Y_oh_train_np = np.array(Y_oh_train)\n",
    "\n",
    "# Stopped here TODO convert Y_train, Y_test to one_hot encodings. This is a binary classification problem.\n",
    "# Y_oh_train = convert_to_one_hot(Y_train, C = 2)\n",
    "# Y_oh_test = convert_to_one_hot(Y_test, C = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence -- string, one training example from X\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (J,), where J can be any number\n",
    "    \"\"\"\n",
    "    # Get a valid word contained in the word_to_vec_map. \n",
    "    any_word = next(iter(word_to_vec_map.keys()))\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Split sentence into list of lower case words (≈ 1 line)\n",
    "    # TODO remove [:5] keeping input to only 5 words now\n",
    "    words = sentence.lower().split() #[:5]\n",
    "\n",
    "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
    "    # Use `np.zeros` and pass in the argument of any word's word 2 vec's shape\n",
    "    avg = np.zeros(word_to_vec_map[any_word].shape)\n",
    "    \n",
    "    # Initialize count to 0\n",
    "    count = 0\n",
    "    \n",
    "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
    "    for w in words:\n",
    "        # Check that word exists in word_to_vec_map\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            # Increment count\n",
    "            count +=1\n",
    "          \n",
    "    if count > 0:\n",
    "        # Get the average. But only if count > 0\n",
    "        avg = avg/count\n",
    "#     print('avg',avg)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "### YOU CANNOT EDIT THIS CELL\n",
    "\n",
    "# BEGIN UNIT TEST\n",
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)\n",
    "\n",
    "def sentence_to_avg_test(target):\n",
    "    # Create a controlled word to vec map\n",
    "    word_to_vec_map = {'a': [3, 3], 'synonym_of_a': [3, 3], 'a_nw': [2, 4], 'a_s': [3, 2], \n",
    "                       'c': [-2, 1], 'c_n': [-2, 2],'c_ne': [-1, 2], 'c_e': [-1, 1], 'c_se': [-1, 0], \n",
    "                       'c_s': [-2, 0], 'c_sw': [-3, 0], 'c_w': [-3, 1], 'c_nw': [-3, 2]\n",
    "                      }\n",
    "    # Convert lists to np.arrays\n",
    "    for key in word_to_vec_map.keys():\n",
    "        word_to_vec_map[key] = np.array(word_to_vec_map[key])\n",
    "        \n",
    "    avg = target(\"a a_nw c_w a_s\", word_to_vec_map)\n",
    "    assert tuple(avg.shape) == tuple(word_to_vec_map['a'].shape),  \"Check the shape of your avg array\"  \n",
    "    assert np.allclose(avg, [1.25, 2.5]),  \"Check that you are finding the 4 words\"\n",
    "    avg = target(\"love a a_nw c_w a_s\", word_to_vec_map)\n",
    "    assert np.allclose(avg, [1.25, 2.5]), \"Divide by count, not len(words)\"\n",
    "    avg = target(\"love\", word_to_vec_map)\n",
    "    assert np.array_equal(avg, [0, 0]), \"Average of no words must give an array of zeros\"\n",
    "    avg = target(\"c_se foo a a_nw c_w a_s deeplearning c_nw\", word_to_vec_map)\n",
    "    assert np.allclose(avg, [0.1666667, 2.0]), \"Debug the last example\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "sentence_to_avg_test(sentence_to_avg)\n",
    "\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data containing sentences, numpy array of shape (m, None)\n",
    "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "    pred -- numpy array of shape (m, 1) with your predictions\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0] \n",
    "    \n",
    "    for j in range(m):                       # Loop over training examples\n",
    "        \n",
    "        # Split jth test example (sentence) into list of lower case words\n",
    "        # TODO remove [:5] keeping input to only 5 words now\n",
    "        words = X[j].lower().split() #[:5]\n",
    "        \n",
    "        # Average words' vectors\n",
    "        avg = np.zeros((n_h,))\n",
    "        count = 0\n",
    "        for w in words:\n",
    "            if w in word_to_vec_map:\n",
    "                avg += word_to_vec_map[w]\n",
    "                count += 1\n",
    "        \n",
    "        if count > 0:\n",
    "            avg = avg / count\n",
    "\n",
    "        # Forward propagation\n",
    "        Z = np.dot(W, avg) + b\n",
    "        \n",
    "#         # Scale Z to prevent numerical instability issues in softmax\n",
    "#         Z *= 0.2\n",
    "#         print('Z',Z)\n",
    "        \n",
    "        A = softmax(Z)\n",
    "        pred[j] = np.argmax(A)\n",
    "#     print('pred[:]',pred[:])\n",
    "#     print('Y',Y)\n",
    "#     print('Y.reshape(Y.shape[0],1)[:]',Y.reshape(Y.shape[0],1)[:])\n",
    "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = [ 120.44504498, -3 ]\n",
    "# My z outputs are too large! Compared to how it was in the Emoji_v3a project\n",
    "# Softmax has a numerical round off issue and returns 0.\n",
    "# Its because the sentences in this dataset are larger.\n",
    "# The max word count from the previous problem is 10 words. Here it is 171!\n",
    "softmax_answer = softmax(test_arr)\n",
    "tanh_answer = tanh(test_arr)\n",
    "print('softmax',softmax_answer)\n",
    "print('tanh',tanh_answer)\n",
    "log_ans = np.log(tanh_answer)\n",
    "print(log_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the high and low of a for original data\n",
    "aOGHigh = []\n",
    "aOGLow = []\n",
    "OG_avgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "aHigh = []\n",
    "aLow = []\n",
    "avgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: model\n",
    "# TODO vectorize this code\n",
    "\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m,)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a valid word contained in the word_to_vec_map \n",
    "    any_word = next(iter(word_to_vec_map.keys()))\n",
    "        \n",
    "    # Define number of training examples\n",
    "#     print('Y.shape',Y.shape)\n",
    "    m = Y.shape[0]                             # number of training examples\n",
    "#     print('Y',Y)\n",
    "    n_y = len(np.unique(Y))                    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0]   # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Original Convert Y to Y_onehot with n_y classes\n",
    "#     Y_oh = convert_to_one_hot(Y, C = n_y)\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    label_encoder = LabelEncoder()\n",
    "#     print('Y before encoding',Y)\n",
    "    Y = label_encoder.fit_transform(Y)\n",
    "#     print('Y after encoding', Y)\n",
    "#     print('Y_vec',Y_vec)\n",
    "    Y_oh = to_categorical(Y) \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations): # Loop over the number of iterations\n",
    "        \n",
    "        cost = 0\n",
    "        dW = 0\n",
    "        db = 0\n",
    "        \n",
    "        for i in range(m):          # Loop over the training examples\n",
    "            \n",
    "            ### START CODE HERE ### (≈ 4 lines of code)\n",
    "            # Average the word vectors of the words from the i'th training example\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "#            print('avg',avg)\n",
    "#             avgs.append(np.max(avg))\n",
    "#             OG_avgs.append(np.max(avg))\n",
    "\n",
    "            # Forward propagate the avg through the softmax layer. \n",
    "            # You can use np.dot() to perform the multiplication.\n",
    "            z = np.dot(W, avg) + b\n",
    "            dropout_mask = np.random.randint(2, size=z.shape)\n",
    "            z *= dropout_mask # Dropout layer start:\n",
    "            \n",
    "#             print('z.shape',z.shape)\n",
    "#             print('z',z)\n",
    "#             zs.append(np.max(z)\n",
    "            \n",
    "#             print('W',W)\n",
    "#             if(z[0] < 0 and z[1] < 0):\n",
    "#                 print('z',z)\n",
    "#             print('z',z)\n",
    "\n",
    "#             # Scale the avg to solve numerical instability problem with softmax\n",
    "#             z *= 0.2\n",
    "        \n",
    "            a = softmax(z)\n",
    "#             aHigh.append(np.max(a))\n",
    "#             aLow.append(np.min(a))\n",
    "            \n",
    "#             aOGHigh.append(np.max(a))\n",
    "#             aOGLow.append(np.min(a))\n",
    "#             print('a',a)\n",
    "#             if(a[0] == 0 or a[1] == 0):\n",
    "#                 print('z',z)\n",
    "#                 print('a',a)\n",
    "\n",
    "            # Add the cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost += -np.sum(Y_oh[i] * np.log(a))\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "            dz *= dropout_mask # Part of Dropout layer: disregard what was dropped out for back prop\n",
    "            dW += np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db += dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "        assert type(cost) == np.float64, \"Incorrect implementation of cost\"\n",
    "        assert cost.shape == (), \"Incorrect implementation of cost\"\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "#             pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "            X_test_np = np.array(X_test)\n",
    "            Y_test_np = np.array(Y_test)\n",
    "#             print('X_test_np',len(X_test_np))\n",
    "#             print('Y_test_np',len(Y_test_np))\n",
    "#             print('Y_test_np',Y_test_np)\n",
    "            label_encoder_1 = LabelEncoder()\n",
    "            Y_test_vec = label_encoder_1.fit_transform(Y_test_np)\n",
    "#             print('Y_test_vec',Y_test_vec)\n",
    "#             print('Y_test_vec',len(Y_test_vec))\n",
    "            pred = predict(X_test_np, Y_test_vec, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-341-e041d9eefe0e>:93: RuntimeWarning: divide by zero encountered in log\n",
      "  cost += -np.sum(Y_oh[i] * np.log(a))\n",
      "<ipython-input-341-e041d9eefe0e>:93: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost += -np.sum(Y_oh[i] * np.log(a))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = nan\n",
      "Accuracy: 0.8492822966507177\n",
      "Epoch: 100 --- cost = nan\n",
      "Accuracy: 0.8744019138755981\n",
      "Epoch: 200 --- cost = nan\n",
      "Accuracy: 0.8875598086124402\n",
      "Epoch: 300 --- cost = nan\n",
      "Accuracy: 0.8660287081339713\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-342-584320f44453>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# print('X_train_np',X_train_np[111])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# print('Y_train_np',Y_train_np[111])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-341-e041d9eefe0e>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X, Y, word_to_vec_map, learning_rate, num_iterations)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;31m# You can use np.dot() to perform the multiplication.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mdropout_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mdropout_mask\u001b[0m \u001b[1;31m# Dropout layer start:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_train_np = np.array(X_train)\n",
    "Y_train_np = np.array(Y_train)\n",
    "# print('X_train_np',X_train_np.shape)\n",
    "# label_encoder = LabelEncoder()\n",
    "# Y_vec = label_encoder.fit_transform(Y_train_np)\n",
    "# Y_oh = to_categorical(Y_vec)\n",
    "# print(Y_train_np[0])\n",
    "# print(Y_oh[0])\n",
    "\n",
    "# print('X_train_np',X_train_np[111])\n",
    "# print('Y_train_np',Y_train_np[111])\n",
    "pred, W, b = model(X_train_np, Y_train_np, word_to_vec_map, learning_rate = 0.01, num_iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_input(X, W, b, word_to_vec_map):\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0] \n",
    "    words = X.lower().split()\n",
    "    avg = np.zeros((n_h,))\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        avg = avg / count\n",
    "\n",
    "    # Forward propagation\n",
    "    z = np.dot(W, avg) + b\n",
    "    a = softmax(z)\n",
    "    pred = np.argmax(a)\n",
    "    return 'ham' if pred == 0 else 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input = 'Dear Voucher Holder, To claim this weeks offer, at you PC please go to http://www.e-tlp.co.uk/expressoffer Ts&Cs apply.'\n",
    "predict_single_input(my_input, W, b, word_to_vec_map)\n",
    "# predict_single_input(my_input,W,b,word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.max(avgs) 2.394430625\n",
      "np.max(OG_avgs) 1.7831400000000002\n"
     ]
    }
   ],
   "source": [
    "# print('np.max aOGHigh',np.max(aOGHigh))\n",
    "# print('np.max aHigh  ',np.max(aHigh))\n",
    "print('np.max(avgs)',np.min(avgs))\n",
    "print('np.max(OG_avgs)',np.min(OG_avgs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
