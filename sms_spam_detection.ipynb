{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load in csv dataset\n",
    "df = pd.read_csv('data/spam.csv',delimiter=',',encoding='latin-1')\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of ham and spam messages')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See output distribution\n",
    "sns.countplot(x=df.v1)\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of ham and spam messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and outputs\n",
    "X = df.v2 #.iloc[:300]\n",
    "Y = df.v1 #.iloc[:300]\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\n",
    "#,Y_train,Y_test = \n",
    "# print(len(X_train))\n",
    "# print(len(Y_train))\n",
    "# print(len(Y_test))\n",
    "# print(len(Y_test)/(len(Y_train)+len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen = len(max(X_train, key=lambda x: len(x.split())).split())\n",
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions taken from emo_utils.py\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    print('Y.reshape(-1)',Y.reshape(-1))\n",
    "    print('C',C)\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "def read_csv(filename = 'data/emojify_data.csv'):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji, dtype=int)\n",
    "\n",
    "    return X[:10], Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OG Data\n",
    "X_train, Y_train = read_csv('og_data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('og_data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen = len(max(X_train, key=lambda x: len(x.split())).split())\n",
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_NP = np.array(Y_train)\n",
    "Y_test_NP = np.array(Y_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "vec = label_encoder.fit_transform(Y_train_NP)\n",
    "print(np.unique(vec))\n",
    "print(np.unique(Y_train_NP))\n",
    "\n",
    "# Y_oh_train = pd.get_dummies(Y_train)\n",
    "# Y_oh_test = pd.get_dummies(Y_test)\n",
    "# Y_oh_train_np = np.array(Y_oh_train)\n",
    "\n",
    "# Stopped here TODO convert Y_train, Y_test to one_hot encodings. This is a binary classification problem.\n",
    "# Y_oh_train = convert_to_one_hot(Y_train, C = 2)\n",
    "# Y_oh_test = convert_to_one_hot(Y_test, C = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence -- string, one training example from X\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (J,), where J can be any number\n",
    "    \"\"\"\n",
    "    # Get a valid word contained in the word_to_vec_map. \n",
    "    any_word = next(iter(word_to_vec_map.keys()))\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Split sentence into list of lower case words (≈ 1 line)\n",
    "    # TODO remove [:5] keeping input to only 5 words now\n",
    "    words = sentence.lower().split() #[:5]\n",
    "\n",
    "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
    "    # Use `np.zeros` and pass in the argument of any word's word 2 vec's shape\n",
    "    avg = np.zeros(word_to_vec_map[any_word].shape)\n",
    "    \n",
    "    # Initialize count to 0\n",
    "    count = 0\n",
    "    \n",
    "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
    "    for w in words:\n",
    "        # Check that word exists in word_to_vec_map\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            # Increment count\n",
    "            count +=1\n",
    "          \n",
    "    if count > 0:\n",
    "        # Get the average. But only if count > 0\n",
    "        avg = avg/count\n",
    "#     print('avg',avg)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "### YOU CANNOT EDIT THIS CELL\n",
    "\n",
    "# BEGIN UNIT TEST\n",
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)\n",
    "\n",
    "def sentence_to_avg_test(target):\n",
    "    # Create a controlled word to vec map\n",
    "    word_to_vec_map = {'a': [3, 3], 'synonym_of_a': [3, 3], 'a_nw': [2, 4], 'a_s': [3, 2], \n",
    "                       'c': [-2, 1], 'c_n': [-2, 2],'c_ne': [-1, 2], 'c_e': [-1, 1], 'c_se': [-1, 0], \n",
    "                       'c_s': [-2, 0], 'c_sw': [-3, 0], 'c_w': [-3, 1], 'c_nw': [-3, 2]\n",
    "                      }\n",
    "    # Convert lists to np.arrays\n",
    "    for key in word_to_vec_map.keys():\n",
    "        word_to_vec_map[key] = np.array(word_to_vec_map[key])\n",
    "        \n",
    "    avg = target(\"a a_nw c_w a_s\", word_to_vec_map)\n",
    "    assert tuple(avg.shape) == tuple(word_to_vec_map['a'].shape),  \"Check the shape of your avg array\"  \n",
    "    assert np.allclose(avg, [1.25, 2.5]),  \"Check that you are finding the 4 words\"\n",
    "    avg = target(\"love a a_nw c_w a_s\", word_to_vec_map)\n",
    "    assert np.allclose(avg, [1.25, 2.5]), \"Divide by count, not len(words)\"\n",
    "    avg = target(\"love\", word_to_vec_map)\n",
    "    assert np.array_equal(avg, [0, 0]), \"Average of no words must give an array of zeros\"\n",
    "    avg = target(\"c_se foo a a_nw c_w a_s deeplearning c_nw\", word_to_vec_map)\n",
    "    assert np.allclose(avg, [0.1666667, 2.0]), \"Debug the last example\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "sentence_to_avg_test(sentence_to_avg)\n",
    "\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data containing sentences, numpy array of shape (m, None)\n",
    "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "    pred -- numpy array of shape (m, 1) with your predictions\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0] \n",
    "    \n",
    "    for j in range(m):                       # Loop over training examples\n",
    "        \n",
    "        # Split jth test example (sentence) into list of lower case words\n",
    "        # TODO remove [:5] keeping input to only 5 words now\n",
    "        words = X[j].lower().split() #[:5]\n",
    "        \n",
    "        # Average words' vectors\n",
    "        avg = np.zeros((n_h,))\n",
    "        count = 0\n",
    "        for w in words:\n",
    "            if w in word_to_vec_map:\n",
    "                avg += word_to_vec_map[w]\n",
    "                count += 1\n",
    "        \n",
    "        if count > 0:\n",
    "            avg = avg / count\n",
    "\n",
    "        # Forward propagation\n",
    "        Z = np.dot(W, avg) + b\n",
    "        \n",
    "#         # Scale Z to prevent numerical instability issues in softmax\n",
    "#         Z *= 0.2\n",
    "#         print('Z',Z)\n",
    "        \n",
    "        A = softmax(Z)\n",
    "        pred[j] = np.argmax(A)\n",
    "#     print('pred[:]',pred[:])\n",
    "#     print('Y',Y)\n",
    "#     print('Y.reshape(Y.shape[0],1)[:]',Y.reshape(Y.shape[0],1)[:])\n",
    "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = [ 120.44504498, -3 ]\n",
    "# My z outputs are too large! Compared to how it was in the Emoji_v3a project\n",
    "# Softmax has a numerical round off issue and returns 0.\n",
    "# Its because the sentences in this dataset are larger.\n",
    "# The max word count from the previous problem is 10 words. Here it is 171!\n",
    "softmax_answer = softmax(test_arr)\n",
    "tanh_answer = tanh(test_arr)\n",
    "print('softmax',softmax_answer)\n",
    "print('tanh',tanh_answer)\n",
    "log_ans = np.log(tanh_answer)\n",
    "print(log_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the high and low of a for original data\n",
    "aOGHigh = []\n",
    "aOGLow = []\n",
    "OG_avgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "aHigh = []\n",
    "aLow = []\n",
    "avgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: model\n",
    "# TODO vectorize this code\n",
    "\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m,)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a valid word contained in the word_to_vec_map \n",
    "    any_word = next(iter(word_to_vec_map.keys()))\n",
    "        \n",
    "    # Define number of training examples\n",
    "#     print('Y.shape',Y.shape)\n",
    "    m = Y.shape[0]                             # number of training examples\n",
    "#     print('Y',Y)\n",
    "    n_y = len(np.unique(Y))                    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0]   # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Original Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y)\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "#     label_encoder = LabelEncoder()\n",
    "# #     print('Y before encoding',Y)\n",
    "#     Y = label_encoder.fit_transform(Y)\n",
    "# #     print('Y after encoding', Y)\n",
    "# #     print('Y_vec',Y_vec)\n",
    "#     Y_oh = to_categorical(Y) \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations): # Loop over the number of iterations\n",
    "        \n",
    "        cost = 0\n",
    "        dW = 0\n",
    "        db = 0\n",
    "        \n",
    "        for i in range(m):          # Loop over the training examples\n",
    "            \n",
    "            ### START CODE HERE ### (≈ 4 lines of code)\n",
    "            # Average the word vectors of the words from the i'th training example\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "#            print('avg',avg)\n",
    "#             avgs.append(np.max(avg))\n",
    "#             OG_avgs.append(np.max(avg))\n",
    "\n",
    "            # Forward propagate the avg through the softmax layer. \n",
    "            # You can use np.dot() to perform the multiplication.\n",
    "            z = np.dot(W, avg) + b\n",
    "            dropout_mask = np.random.randint(2, size=z.shape)\n",
    "#             z *= dropout_mask # Dropout layer start:\n",
    "            \n",
    "#             print('z.shape',z.shape)\n",
    "#             print('z',z)\n",
    "#             zs.append(np.max(z)\n",
    "            \n",
    "#             print('W',W)\n",
    "#             if(z[0] < 0 and z[1] < 0):\n",
    "#                 print('z',z)\n",
    "#             print('z',z)\n",
    "\n",
    "#             # Scale the avg to solve numerical instability problem with softmax\n",
    "#             z *= 0.2\n",
    "        \n",
    "            a = softmax(z)\n",
    "#             aHigh.append(np.max(a))\n",
    "#             aLow.append(np.min(a))\n",
    "            \n",
    "#             aOGHigh.append(np.max(a))\n",
    "#             aOGLow.append(np.min(a))\n",
    "#             print('a',a)\n",
    "#             if(a[0] == 0 or a[1] == 0):\n",
    "#                 print('z',z)\n",
    "#                 print('a',a)\n",
    "\n",
    "            # Add the cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost += -np.sum(Y_oh[i] * np.log(a))\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "#             dz *= dropout_mask # Dropout layer back prop: disregard what was dropped out for back prop\n",
    "            dW += np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db += dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "        assert type(cost) == np.float64, \"Incorrect implementation of cost\"\n",
    "        assert cost.shape == (), \"Incorrect implementation of cost\"\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "\n",
    "#             X_test_np = np.array(X_test)\n",
    "#             Y_test_np = np.array(Y_test)\n",
    "\n",
    "#             print('X_test_np',len(X_test_np))\n",
    "#             print('Y_test_np',len(Y_test_np))\n",
    "#             print('Y_test_np',Y_test_np)\n",
    "\n",
    "#             label_encoder_1 = LabelEncoder()\n",
    "#             Y_test_vec = label_encoder_1.fit_transform(Y_test_np)\n",
    "\n",
    "#             print('Y_test_vec',Y_test_vec)\n",
    "#             print('Y_test_vec',len(Y_test_vec))\n",
    "            \n",
    "#             pred = predict(X_test_np, Y_test_vec, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.reshape(-1) [3 2 3 0 4 0 3 2 3 1]\n",
      "C 5\n",
      "Epoch: 0 --- cost = 20.210659714307834\n",
      "Accuracy: 0.4\n",
      "Epoch: 100 --- cost = 1.9523038177417518\n",
      "Accuracy: 0.9\n",
      "Epoch: 200 --- cost = 0.8601801121028543\n",
      "Accuracy: 1.0\n",
      "Epoch: 300 --- cost = 0.5218621978061981\n",
      "Accuracy: 1.0\n",
      "Epoch: 400 --- cost = 0.3695802894118113\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_train_np = np.array(X_train)\n",
    "Y_train_np = np.array(Y_train)\n",
    "# print('X_train_np',X_train_np.shape)\n",
    "# label_encoder = LabelEncoder()\n",
    "# Y_vec = label_encoder.fit_transform(Y_train_np)\n",
    "# Y_oh = to_categorical(Y_vec)\n",
    "# print(Y_train_np[0])\n",
    "# print(Y_oh[0])\n",
    "\n",
    "# print('X_train_np',X_train_np[111])\n",
    "# print('Y_train_np',Y_train_np[111])\n",
    "pred, W, b = model(X_train_np, Y_train_np, word_to_vec_map, learning_rate = 0.01, num_iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_input(X, W, b, word_to_vec_map):\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0] \n",
    "    words = X.lower().split()\n",
    "    avg = np.zeros((n_h,))\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        avg = avg / count\n",
    "\n",
    "    # Forward propagation\n",
    "    z = np.dot(W, avg) + b\n",
    "    a = softmax(z)\n",
    "    pred = np.argmax(a)\n",
    "    return 'ham' if pred == 0 else 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input = 'Dear Voucher Holder, To claim this weeks offer, at you PC please go to http://www.e-tlp.co.uk/expressoffer Ts&Cs apply.'\n",
    "predict_single_input(my_input, W, b, word_to_vec_map)\n",
    "# predict_single_input(my_input,W,b,word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.max(avgs) 2.394430625\n",
      "np.max(OG_avgs) 1.7831400000000002\n"
     ]
    }
   ],
   "source": [
    "# print('np.max aOGHigh',np.max(aOGHigh))\n",
    "# print('np.max aHigh  ',np.max(aHigh))\n",
    "print('np.max(avgs)',np.min(avgs))\n",
    "print('np.max(OG_avgs)',np.min(OG_avgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.94774251, -15.72348395, -17.63724974, -12.29135768,\n",
       "          5.35669997, -39.54165892,  11.73299218,  38.01036993,\n",
       "         14.58554341,  -6.35013144,  -4.59018494,   5.63928006,\n",
       "        -37.15148319, -17.14549208,  -7.01384417,  18.18544452,\n",
       "        -10.8547211 ,   8.33817139,  18.53098003, -21.16736835,\n",
       "          5.36325393,  14.4002783 ,  -5.05654081, -25.63397883,\n",
       "          9.74544027, -12.32893737, -12.45023846,  33.02010502,\n",
       "         -3.87371054,  23.78670648, -14.31305574,   1.5031102 ,\n",
       "         12.52826313, -15.06762   ,  17.96334211, -10.51625451,\n",
       "          6.15298609,  18.64807106,  32.43649156,  21.54501196,\n",
       "        -48.23225419,  16.50552861, -40.55139139,  -7.39925995,\n",
       "        -22.83503621,  -0.31998146,  -5.49498431,  -8.01198582,\n",
       "         13.82494765,  29.06043629],\n",
       "       [ -1.03425826,  16.45883929,  22.58671672,  17.80421241,\n",
       "         -8.36424462,  35.32581316, -15.53267933, -36.26025801,\n",
       "         -8.03131127,  -1.35185559,   5.01130996,  -3.76737099,\n",
       "         36.47326676,  20.66605725,  -0.68109532, -12.5696324 ,\n",
       "         11.92049322,  -9.43503539, -17.41878267,  24.01710615,\n",
       "         -5.02643345, -15.147541  ,  -2.79344409,  20.62457842,\n",
       "        -15.07369895,  15.17085639,  11.73487577, -41.48803793,\n",
       "          3.45578209, -11.89442435,  -3.65554362,  -0.51152406,\n",
       "        -16.54934804,  16.91107148, -13.43419899,  12.7914415 ,\n",
       "         -6.70658605, -26.08628654, -34.07297056, -18.92186817,\n",
       "         48.64760749, -10.9735268 ,  36.11175644,   8.15153228,\n",
       "         24.08776478,   2.4048549 ,   1.20324739,  16.77643886,\n",
       "        -16.9468414 , -26.65894853]])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sms_W = W\n",
    "sms_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# emoji_W = W\n",
    "emoji_W\n",
    "sns.heatmap(emoji_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.32541529, -8.1972296 ])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sms_b = b\n",
    "sms_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22099683, -0.34913181,  0.2686692 ,  0.30064856,  0.00081087])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emoji_b = b\n",
    "emoji_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
