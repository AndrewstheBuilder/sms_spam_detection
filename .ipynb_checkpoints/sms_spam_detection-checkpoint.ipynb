{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load in csv dataset\n",
    "df = pd.read_csv('data/spam.csv',delimiter=',',encoding='latin-1')\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of ham and spam messages')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See output distribution\n",
    "sns.countplot(x=df.v1)\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of ham and spam messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and outputs\n",
    "X = df.v2 #.iloc[:300]\n",
    "Y = df.v1 #.iloc[:300]\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\n",
    "#,Y_train,Y_test = \n",
    "# print(len(X_train))\n",
    "# print(len(Y_train))\n",
    "# print(len(Y_test))\n",
    "# print(len(Y_test)/(len(Y_train)+len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen = len(max(X_train, key=lambda x: len(x.split())).split())\n",
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions taken from emo_utils.py\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    print('Y.reshape(-1)',Y.reshape(-1))\n",
    "    print('C',C)\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "def read_csv(filename = 'data/emojify_data.csv'):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji, dtype=int)\n",
    "\n",
    "    return X[:10], Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OG Data\n",
    "X_train, Y_train = read_csv('og_data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('og_data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen = len(max(X_train, key=lambda x: len(x.split())).split())\n",
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_NP = np.array(Y_train)\n",
    "Y_test_NP = np.array(Y_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "vec = label_encoder.fit_transform(Y_train_NP)\n",
    "print(np.unique(vec))\n",
    "print(np.unique(Y_train_NP))\n",
    "\n",
    "# Y_oh_train = pd.get_dummies(Y_train)\n",
    "# Y_oh_test = pd.get_dummies(Y_test)\n",
    "# Y_oh_train_np = np.array(Y_oh_train)\n",
    "\n",
    "# Stopped here TODO convert Y_train, Y_test to one_hot encodings. This is a binary classification problem.\n",
    "# Y_oh_train = convert_to_one_hot(Y_train, C = 2)\n",
    "# Y_oh_test = convert_to_one_hot(Y_test, C = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence -- string, one training example from X\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (J,), where J can be any number\n",
    "    \"\"\"\n",
    "    # Get a valid word contained in the word_to_vec_map. \n",
    "    any_word = next(iter(word_to_vec_map.keys()))\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Split sentence into list of lower case words (≈ 1 line)\n",
    "    # TODO remove [:5] keeping input to only 5 words now\n",
    "    words = sentence.lower().split() #[:5]\n",
    "\n",
    "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
    "    # Use `np.zeros` and pass in the argument of any word's word 2 vec's shape\n",
    "    avg = np.zeros(word_to_vec_map[any_word].shape)\n",
    "    \n",
    "    # Initialize count to 0\n",
    "    count = 0\n",
    "    \n",
    "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
    "    for w in words:\n",
    "        # Check that word exists in word_to_vec_map\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            # Increment count\n",
    "            count +=1\n",
    "          \n",
    "    if count > 0:\n",
    "        # Get the average. But only if count > 0\n",
    "        avg = avg/count\n",
    "#     print('avg',avg)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "### YOU CANNOT EDIT THIS CELL\n",
    "\n",
    "# BEGIN UNIT TEST\n",
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)\n",
    "\n",
    "def sentence_to_avg_test(target):\n",
    "    # Create a controlled word to vec map\n",
    "    word_to_vec_map = {'a': [3, 3], 'synonym_of_a': [3, 3], 'a_nw': [2, 4], 'a_s': [3, 2], \n",
    "                       'c': [-2, 1], 'c_n': [-2, 2],'c_ne': [-1, 2], 'c_e': [-1, 1], 'c_se': [-1, 0], \n",
    "                       'c_s': [-2, 0], 'c_sw': [-3, 0], 'c_w': [-3, 1], 'c_nw': [-3, 2]\n",
    "                      }\n",
    "    # Convert lists to np.arrays\n",
    "    for key in word_to_vec_map.keys():\n",
    "        word_to_vec_map[key] = np.array(word_to_vec_map[key])\n",
    "        \n",
    "    avg = target(\"a a_nw c_w a_s\", word_to_vec_map)\n",
    "    assert tuple(avg.shape) == tuple(word_to_vec_map['a'].shape),  \"Check the shape of your avg array\"  \n",
    "    assert np.allclose(avg, [1.25, 2.5]),  \"Check that you are finding the 4 words\"\n",
    "    avg = target(\"love a a_nw c_w a_s\", word_to_vec_map)\n",
    "    assert np.allclose(avg, [1.25, 2.5]), \"Divide by count, not len(words)\"\n",
    "    avg = target(\"love\", word_to_vec_map)\n",
    "    assert np.array_equal(avg, [0, 0]), \"Average of no words must give an array of zeros\"\n",
    "    avg = target(\"c_se foo a a_nw c_w a_s deeplearning c_nw\", word_to_vec_map)\n",
    "    assert np.allclose(avg, [0.1666667, 2.0]), \"Debug the last example\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "sentence_to_avg_test(sentence_to_avg)\n",
    "\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data containing sentences, numpy array of shape (m, None)\n",
    "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "    pred -- numpy array of shape (m, 1) with your predictions\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0] \n",
    "    \n",
    "    for j in range(m):                       # Loop over training examples\n",
    "        \n",
    "        # Split jth test example (sentence) into list of lower case words\n",
    "        # TODO remove [:5] keeping input to only 5 words now\n",
    "        words = X[j].lower().split() #[:5]\n",
    "        \n",
    "        # Average words' vectors\n",
    "        avg = np.zeros((n_h,))\n",
    "        count = 0\n",
    "        for w in words:\n",
    "            if w in word_to_vec_map:\n",
    "                avg += word_to_vec_map[w]\n",
    "                count += 1\n",
    "        \n",
    "        if count > 0:\n",
    "            avg = avg / count\n",
    "\n",
    "        # Forward propagation\n",
    "        Z = np.dot(W, avg) + b\n",
    "        \n",
    "#         # Scale Z to prevent numerical instability issues in softmax\n",
    "#         Z *= 0.2\n",
    "#         print('Z',Z)\n",
    "        \n",
    "        A = softmax(Z)\n",
    "        pred[j] = np.argmax(A)\n",
    "#     print('pred[:]',pred[:])\n",
    "#     print('Y',Y)\n",
    "#     print('Y.reshape(Y.shape[0],1)[:]',Y.reshape(Y.shape[0],1)[:])\n",
    "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = [ 120.44504498, -3 ]\n",
    "# My z outputs are too large! Compared to how it was in the Emoji_v3a project\n",
    "# Softmax has a numerical round off issue and returns 0.\n",
    "# Its because the sentences in this dataset are larger.\n",
    "# The max word count from the previous problem is 10 words. Here it is 171!\n",
    "softmax_answer = softmax(test_arr)\n",
    "tanh_answer = tanh(test_arr)\n",
    "print('softmax',softmax_answer)\n",
    "print('tanh',tanh_answer)\n",
    "log_ans = np.log(tanh_answer)\n",
    "print(log_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the high and low of a for original data\n",
    "aOGHigh = []\n",
    "aOGLow = []\n",
    "OG_avgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "aHigh = []\n",
    "aLow = []\n",
    "avgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: model\n",
    "# TODO vectorize this code\n",
    "\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m,)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a valid word contained in the word_to_vec_map \n",
    "    any_word = next(iter(word_to_vec_map.keys()))\n",
    "        \n",
    "    # Define number of training examples\n",
    "#     print('Y.shape',Y.shape)\n",
    "    m = Y.shape[0]                             # number of training examples\n",
    "#     print('Y',Y)\n",
    "    n_y = len(np.unique(Y))                    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0]   # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Original Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y)\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "#     label_encoder = LabelEncoder()\n",
    "# #     print('Y before encoding',Y)\n",
    "#     Y = label_encoder.fit_transform(Y)\n",
    "# #     print('Y after encoding', Y)\n",
    "# #     print('Y_vec',Y_vec)\n",
    "#     Y_oh = to_categorical(Y) \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations): # Loop over the number of iterations\n",
    "        \n",
    "        cost = 0\n",
    "        dW = 0\n",
    "        db = 0\n",
    "        \n",
    "        for i in range(m):          # Loop over the training examples\n",
    "            \n",
    "            ### START CODE HERE ### (≈ 4 lines of code)\n",
    "            # Average the word vectors of the words from the i'th training example\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "#            print('avg',avg)\n",
    "#             avgs.append(np.max(avg))\n",
    "#             OG_avgs.append(np.max(avg))\n",
    "\n",
    "            # Forward propagate the avg through the softmax layer. \n",
    "            # You can use np.dot() to perform the multiplication.\n",
    "            z = np.dot(W, avg) + b\n",
    "            dropout_mask = np.random.randint(2, size=z.shape)\n",
    "#             z *= dropout_mask # Dropout layer start:\n",
    "            \n",
    "#             print('z.shape',z.shape)\n",
    "#             print('z',z)\n",
    "#             zs.append(np.max(z)\n",
    "            \n",
    "#             print('W',W)\n",
    "#             if(z[0] < 0 and z[1] < 0):\n",
    "#                 print('z',z)\n",
    "#             print('z',z)\n",
    "\n",
    "#             # Scale the avg to solve numerical instability problem with softmax\n",
    "#             z *= 0.2\n",
    "        \n",
    "            a = softmax(z)\n",
    "#             aHigh.append(np.max(a))\n",
    "#             aLow.append(np.min(a))\n",
    "            \n",
    "#             aOGHigh.append(np.max(a))\n",
    "#             aOGLow.append(np.min(a))\n",
    "#             print('a',a)\n",
    "#             if(a[0] == 0 or a[1] == 0):\n",
    "#                 print('z',z)\n",
    "#                 print('a',a)\n",
    "\n",
    "            # Add the cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost += -np.sum(Y_oh[i] * np.log(a))\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "#             dz *= dropout_mask # Dropout layer back prop: disregard what was dropped out for back prop\n",
    "            dW += np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db += dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "        assert type(cost) == np.float64, \"Incorrect implementation of cost\"\n",
    "        assert cost.shape == (), \"Incorrect implementation of cost\"\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "\n",
    "#             X_test_np = np.array(X_test)\n",
    "#             Y_test_np = np.array(Y_test)\n",
    "\n",
    "#             print('X_test_np',len(X_test_np))\n",
    "#             print('Y_test_np',len(Y_test_np))\n",
    "#             print('Y_test_np',Y_test_np)\n",
    "\n",
    "#             label_encoder_1 = LabelEncoder()\n",
    "#             Y_test_vec = label_encoder_1.fit_transform(Y_test_np)\n",
    "\n",
    "#             print('Y_test_vec',Y_test_vec)\n",
    "#             print('Y_test_vec',len(Y_test_vec))\n",
    "            \n",
    "#             pred = predict(X_test_np, Y_test_vec, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.reshape(-1) [3 2 3 0 4 0 3 2 3 1]\n",
      "C 5\n",
      "Epoch: 0 --- cost = 20.210659714307834\n",
      "Accuracy: 0.4\n",
      "Epoch: 100 --- cost = 1.9523038177417518\n",
      "Accuracy: 0.9\n",
      "Epoch: 200 --- cost = 0.8601801121028543\n",
      "Accuracy: 1.0\n",
      "Epoch: 300 --- cost = 0.5218621978061981\n",
      "Accuracy: 1.0\n",
      "Epoch: 400 --- cost = 0.3695802894118113\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_train_np = np.array(X_train)\n",
    "Y_train_np = np.array(Y_train)\n",
    "# print('X_train_np',X_train_np.shape)\n",
    "# label_encoder = LabelEncoder()\n",
    "# Y_vec = label_encoder.fit_transform(Y_train_np)\n",
    "# Y_oh = to_categorical(Y_vec)\n",
    "# print(Y_train_np[0])\n",
    "# print(Y_oh[0])\n",
    "\n",
    "# print('X_train_np',X_train_np[111])\n",
    "# print('Y_train_np',Y_train_np[111])\n",
    "pred, W, b = model(X_train_np, Y_train_np, word_to_vec_map, learning_rate = 0.01, num_iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_input(X, W, b, word_to_vec_map):\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0] \n",
    "    words = X.lower().split()\n",
    "    avg = np.zeros((n_h,))\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        avg = avg / count\n",
    "\n",
    "    # Forward propagation\n",
    "    z = np.dot(W, avg) + b\n",
    "    a = softmax(z)\n",
    "    pred = np.argmax(a)\n",
    "    return 'ham' if pred == 0 else 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input = 'Dear Voucher Holder, To claim this weeks offer, at you PC please go to http://www.e-tlp.co.uk/expressoffer Ts&Cs apply.'\n",
    "predict_single_input(my_input, W, b, word_to_vec_map)\n",
    "# predict_single_input(my_input,W,b,word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.max(avgs) 2.394430625\n",
      "np.max(OG_avgs) 1.7831400000000002\n"
     ]
    }
   ],
   "source": [
    "# print('np.max aOGHigh',np.max(aOGHigh))\n",
    "# print('np.max aHigh  ',np.max(aHigh))\n",
    "print('np.max(avgs)',np.min(avgs))\n",
    "print('np.max(OG_avgs)',np.min(OG_avgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.94774251, -15.72348395, -17.63724974, -12.29135768,\n",
       "          5.35669997, -39.54165892,  11.73299218,  38.01036993,\n",
       "         14.58554341,  -6.35013144,  -4.59018494,   5.63928006,\n",
       "        -37.15148319, -17.14549208,  -7.01384417,  18.18544452,\n",
       "        -10.8547211 ,   8.33817139,  18.53098003, -21.16736835,\n",
       "          5.36325393,  14.4002783 ,  -5.05654081, -25.63397883,\n",
       "          9.74544027, -12.32893737, -12.45023846,  33.02010502,\n",
       "         -3.87371054,  23.78670648, -14.31305574,   1.5031102 ,\n",
       "         12.52826313, -15.06762   ,  17.96334211, -10.51625451,\n",
       "          6.15298609,  18.64807106,  32.43649156,  21.54501196,\n",
       "        -48.23225419,  16.50552861, -40.55139139,  -7.39925995,\n",
       "        -22.83503621,  -0.31998146,  -5.49498431,  -8.01198582,\n",
       "         13.82494765,  29.06043629],\n",
       "       [ -1.03425826,  16.45883929,  22.58671672,  17.80421241,\n",
       "         -8.36424462,  35.32581316, -15.53267933, -36.26025801,\n",
       "         -8.03131127,  -1.35185559,   5.01130996,  -3.76737099,\n",
       "         36.47326676,  20.66605725,  -0.68109532, -12.5696324 ,\n",
       "         11.92049322,  -9.43503539, -17.41878267,  24.01710615,\n",
       "         -5.02643345, -15.147541  ,  -2.79344409,  20.62457842,\n",
       "        -15.07369895,  15.17085639,  11.73487577, -41.48803793,\n",
       "          3.45578209, -11.89442435,  -3.65554362,  -0.51152406,\n",
       "        -16.54934804,  16.91107148, -13.43419899,  12.7914415 ,\n",
       "         -6.70658605, -26.08628654, -34.07297056, -18.92186817,\n",
       "         48.64760749, -10.9735268 ,  36.11175644,   8.15153228,\n",
       "         24.08776478,   2.4048549 ,   1.20324739,  16.77643886,\n",
       "        -16.9468414 , -26.65894853]])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sms_W = W\n",
    "sms_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.95862193e-01, -2.36616359e-01,  1.19074691e-01,\n",
       "        -1.64230771e-01,  9.10198679e-02,  9.18290384e-02,\n",
       "         1.92524722e-01, -2.49092455e-01,  4.63480483e-01,\n",
       "         5.10187612e-01, -2.45149572e-01,  3.49034396e-01,\n",
       "         4.16142352e-01,  3.11838394e-02,  1.07078534e+00,\n",
       "         5.14799385e-01,  3.34278112e-01,  7.82977006e-02,\n",
       "         3.95467970e-01, -9.95774343e-01, -1.33941262e+00,\n",
       "         3.48894227e-01,  7.98787055e-01,  4.42400066e-01,\n",
       "         5.58692222e-01,  4.46919854e-01, -7.29007853e-01,\n",
       "         1.11304323e+00,  9.20684688e-01, -1.31819123e+00,\n",
       "        -1.28192990e+00,  1.99500728e-01,  1.05340618e+00,\n",
       "         6.05081256e-01,  2.49354887e-01,  6.78470853e-02,\n",
       "        -6.00611557e-01,  5.78316734e-01,  6.53091014e-01,\n",
       "        -1.39444438e+00, -2.37549281e-01,  5.18837878e-01,\n",
       "         3.07168632e-01,  5.72991496e-01, -2.23629347e-01,\n",
       "        -2.50224750e-01,  7.55033484e-01, -9.45672709e-01,\n",
       "        -2.74949935e-01,  1.12457829e+00],\n",
       "       [-1.33792580e-01, -4.31374654e-01,  6.57992091e-02,\n",
       "         2.80071665e-01, -2.87500180e-01, -3.99311948e-01,\n",
       "        -2.11319527e-02,  5.89254542e-01, -6.38194231e-01,\n",
       "         5.27254475e-02, -8.30347254e-02,  2.70649770e-01,\n",
       "        -2.88593805e-01,  3.09581121e-01, -6.95698070e-04,\n",
       "         3.53566132e-01,  1.15829082e+00, -4.55570743e-01,\n",
       "         1.98198402e-01, -4.23178642e-01, -1.49169912e-01,\n",
       "        -5.09373521e-01, -2.22631991e-01,  7.12755077e-01,\n",
       "        -4.64150560e-01,  7.87740909e-02,  6.66139259e-01,\n",
       "        -9.50636502e-01, -2.10021459e-01, -7.75366104e-01,\n",
       "        -7.44882214e-02,  7.32745403e-01, -8.30530827e-01,\n",
       "         3.46878624e-01,  5.20657429e-01,  2.94222604e-01,\n",
       "         7.81808143e-02,  1.04079209e-01,  3.21837546e-01,\n",
       "         3.45943130e-01,  1.87375200e-01,  1.12231708e-01,\n",
       "         4.19308408e-01,  2.63105518e-02,  3.10719421e-01,\n",
       "         5.01690409e-01,  2.98507001e-01,  3.03888840e-01,\n",
       "        -1.99831437e-01,  9.01138639e-02],\n",
       "       [-5.58482401e-01,  1.63242178e+00,  2.93474589e-02,\n",
       "        -2.93292641e-01,  8.61516856e-01, -9.01926045e-01,\n",
       "         8.58403244e-01, -6.26608545e-02, -2.07422656e-01,\n",
       "        -9.43407541e-02, -1.28246007e-01, -1.53885901e-01,\n",
       "         3.87509204e-01, -6.09351913e-01, -1.00574025e+00,\n",
       "        -8.78573865e-01, -5.70396895e-01,  5.57942948e-01,\n",
       "         8.16486558e-01,  6.01586814e-02, -1.02601628e-01,\n",
       "         6.61439651e-02, -2.69749441e-01, -4.93536660e-01,\n",
       "         6.75119496e-01,  3.38054792e-01, -1.01536076e+00,\n",
       "        -6.94463230e-01, -8.30420788e-01,  7.07270902e-01,\n",
       "        -2.49418835e-02,  1.20556997e+00, -4.24659808e-01,\n",
       "         6.47155214e-02, -2.79614673e-01, -3.16627131e-01,\n",
       "         2.78454793e-01,  3.15009758e-01, -6.36696884e-01,\n",
       "        -1.39852443e-02,  7.83744171e-01, -1.19420369e+00,\n",
       "        -4.02087638e-01,  4.37954158e-01, -4.96608619e-01,\n",
       "         2.39985693e-01, -5.34200219e-01,  1.03918371e+00,\n",
       "         5.14882088e-02, -2.65713406e-03],\n",
       "       [ 1.06400315e-01, -1.31133853e+00,  3.36095984e-01,\n",
       "         2.38173369e-01, -1.00613826e+00,  4.68668679e-01,\n",
       "        -4.15240208e-01,  6.34094955e-01, -8.78442335e-01,\n",
       "        -8.19362821e-01, -1.11367547e-01, -8.76629902e-01,\n",
       "        -1.32329969e+00,  1.30196881e-01,  1.62539819e-01,\n",
       "        -5.45445491e-01, -8.75299033e-01, -5.69580718e-01,\n",
       "        -1.21753751e+00,  8.94649208e-01,  7.28111562e-01,\n",
       "         2.79754546e-01,  5.81407040e-01, -5.34060282e-01,\n",
       "        -3.75779876e-01, -1.52425237e+00,  1.31410415e+00,\n",
       "         6.74045104e-01, -2.23288979e-01,  7.68124368e-01,\n",
       "         7.02117880e-01, -1.63478492e+00, -2.39954696e-01,\n",
       "        -6.21024954e-01, -4.87035950e-01, -6.92213212e-01,\n",
       "         6.54370977e-01, -1.17968903e+00, -4.81374933e-01,\n",
       "         8.99978033e-01, -5.94837330e-01, -6.12632230e-03,\n",
       "        -5.30662368e-01, -5.44109962e-01, -4.79293450e-02,\n",
       "        -1.13248275e+00,  7.23444645e-03,  2.01948184e-01,\n",
       "         1.69909315e-01, -9.67482366e-01],\n",
       "       [ 4.98396690e-01,  1.55942166e-01, -6.62136259e-01,\n",
       "         4.07824995e-01,  1.53154764e-01,  2.59005840e-01,\n",
       "        -3.97164930e-01, -1.16522340e+00,  1.15263209e+00,\n",
       "         5.63268398e-01,  3.54392843e-01,  1.03524094e-02,\n",
       "         1.14763275e+00, -2.04532424e-01, -5.27724391e-02,\n",
       "         3.02988942e-01,  3.10951742e-01,  2.19878812e-01,\n",
       "         7.89901898e-01,  1.56972262e-01,  6.36895014e-01,\n",
       "        -5.90969545e-03,  2.04681135e-02,  3.25914035e-01,\n",
       "        -6.50923985e-01,  7.67034841e-01,  4.47390409e-02,\n",
       "        -1.78182400e-01,  1.65224733e-01,  8.49034678e-01,\n",
       "         3.78750238e-01, -6.00053910e-01,  3.61628994e-01,\n",
       "         5.70064474e-02,  9.21757513e-03,  3.11931624e-01,\n",
       "        -6.77250526e-01,  4.68894437e-02,  3.11428129e-01,\n",
       "         5.40105253e-01,  7.09438548e-02,  4.63801011e-01,\n",
       "         4.39506500e-01, -2.78387718e-01,  5.70797230e-01,\n",
       "         4.46622126e-01, -3.64542417e-01, -2.11946489e-01,\n",
       "         5.86309414e-01,  1.02577530e-03]])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emoji_W = W\n",
    "emoji_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.32541529, -8.1972296 ])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sms_b = b\n",
    "sms_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22099683, -0.34913181,  0.2686692 ,  0.30064856,  0.00081087])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emoji_b = b\n",
    "emoji_b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
